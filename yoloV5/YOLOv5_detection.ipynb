{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appreciated-cooperative",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['./runs/train/yolo_v5s_pretrained/weights/best.pt'], source=./datasets/custom_data/belona_test01.mp4, imgsz=640, conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=yolo_v5s_pretrained_belona01, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /notebooks/RIPCurrentDetection/requirements.txt not found, check failed.\n",
      "YOLOv5 ðŸš€ v5.0-351-ge96c74b torch 1.8.0a0+52ea372 CUDA:0 (Quadro P5000, 16278.5625MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7053910 parameters, 0 gradients\n",
      "Traceback (most recent call last):\n",
      "  File \"./yolov5/detect.py\", line 238, in <module>\n",
      "    main(opt)\n",
      "  File \"./yolov5/detect.py\", line 233, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"./yolov5/detect.py\", line 98, in run\n",
      "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 881, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/notebooks/RIPCurrentDetection/yolov5/models/yolo.py\", line 122, in forward\n",
      "    return self.forward_once(x, profile, visualize)  # single-scale inference, train\n",
      "  File \"/notebooks/RIPCurrentDetection/yolov5/models/yolo.py\", line 153, in forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 881, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/notebooks/RIPCurrentDetection/yolov5/models/common.py\", line 176, in forward\n",
      "    return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 58.52 MiB already allocated; 4.50 MiB free; 66.00 MiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "# YOLO detection on video\n",
    "!python ./yolov5/detect.py --source ./datasets/custom_data/belona_test01.mp4 --weights ./runs/train/yolo_v5s_pretrained/weights/best.pt --conf 0.1 --name yolo_v5s_pretrained_belona01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO detection on video \n",
    "!python ./yolov5/detect.py --source ./datasets/custom_data/belona_test01.mp4 --weights ./runs/train/yolo_v5s_custom/weights/best.pt --conf 0.1 --name yolo_v5s_custom_belona_test01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO detection on video \n",
    "!python ./yolov5/detect.py --source ./datasets/custom_data/belona_test03.mp4 --weights ./runs/train/yolo_v5s_pretrained/weights/best.pt --conf 0.1 --name yolo_v5s_pretrained_belona_test03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
